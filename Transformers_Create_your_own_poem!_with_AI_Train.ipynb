{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers: Create your own poem! with AI_Train.ipynb",
      "provenance": [],
      "mount_file_id": "1xWtxFtplWzx7f6ZTEWcjLshe7KXwmPA8",
      "authorship_tag": "ABX9TyM4WWQtOkgHG0oMCMVEImXa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ancastillar/Transformers-Create-your-own-poem-/blob/main/Transformers_Create_your_own_poem!_with_AI_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import libraries"
      ],
      "metadata": {
        "id": "4B9XFhexfkY5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Abywl7XSNs8",
        "outputId": "d72f417a-aa99-403d-8f45-21539712253b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting aitextgen\n",
            "  Downloading aitextgen-0.5.2.tar.gz (572 kB)\n",
            "\u001b[K     |████████████████████████████████| 572 kB 7.4 MB/s \n",
            "\u001b[?25hCollecting transformers>=4.5.1\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 52.7 MB/s \n",
            "\u001b[?25hCollecting fire>=0.3.0\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 8.2 MB/s \n",
            "\u001b[?25hCollecting pytorch-lightning>=1.3.1\n",
            "  Downloading pytorch_lightning-1.6.4-py3-none-any.whl (585 kB)\n",
            "\u001b[K     |████████████████████████████████| 585 kB 15.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from aitextgen) (1.11.0+cu113)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fire>=0.3.0->aitextgen) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire>=0.3.0->aitextgen) (1.1.0)\n",
            "Requirement already satisfied: protobuf<=3.20.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.3.1->aitextgen) (3.17.3)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.3.1->aitextgen) (4.1.1)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.3.1->aitextgen) (2.8.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.3.1->aitextgen) (21.3)\n",
            "Collecting pyDeprecate>=0.3.1\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Collecting PyYAML>=5.4\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 56.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.3.1->aitextgen) (1.21.6)\n",
            "Collecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.9.2-py3-none-any.whl (419 kB)\n",
            "\u001b[K     |████████████████████████████████| 419 kB 49.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.3.1->aitextgen) (4.64.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 46.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.3.1->aitextgen) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning>=1.3.1->aitextgen) (3.0.9)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (3.3.7)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (1.35.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (0.37.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (1.46.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (1.1.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.3.1->aitextgen) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.3.1->aitextgen) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.3.1->aitextgen) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.3.1->aitextgen) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (3.2.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 39.5 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 13.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.5.1->aitextgen) (3.7.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.5.1->aitextgen) (2022.6.2)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.3.1->aitextgen) (21.4.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 58.4 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 2.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.3.1->aitextgen) (2.0.12)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 49.8 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Building wheels for collected packages: aitextgen, fire\n",
            "  Building wheel for aitextgen (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for aitextgen: filename=aitextgen-0.5.2-py3-none-any.whl size=575905 sha256=708280be7d74fad6d99ce6a7691415d51bc9a52bacccd7cda342cdca72f57c34\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/e2/74/46c887b0989a51a7acee0c09551a3ae9d34b939fb4bea404a0\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=f84500b1b77f37b6e07ee0ae7131f45ba60039ed29cc156addf4be8c13fac510\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
            "Successfully built aitextgen fire\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, PyYAML, fsspec, aiohttp, torchmetrics, tokenizers, pyDeprecate, huggingface-hub, transformers, pytorch-lightning, fire, aitextgen\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-6.0 aiohttp-3.8.1 aiosignal-1.2.0 aitextgen-0.5.2 async-timeout-4.0.2 asynctest-0.13.0 fire-0.4.0 frozenlist-1.3.0 fsspec-2022.5.0 huggingface-hub-0.8.1 multidict-6.0.2 pyDeprecate-0.3.2 pytorch-lightning-1.6.4 tokenizers-0.12.1 torchmetrics-0.9.2 transformers-4.20.1 yarl-1.7.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask-ngrok) (2.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2022.6.15)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting flask==0.12.2\n",
            "  Downloading Flask-0.12.2-py2.py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=2.0 in /usr/local/lib/python3.7/dist-packages (from flask==0.12.2) (7.1.2)\n",
            "Requirement already satisfied: Jinja2>=2.4 in /usr/local/lib/python3.7/dist-packages (from flask==0.12.2) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.7/dist-packages (from flask==0.12.2) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous>=0.21 in /usr/local/lib/python3.7/dist-packages (from flask==0.12.2) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.4->flask==0.12.2) (2.0.1)\n",
            "Installing collected packages: flask\n",
            "  Attempting uninstall: flask\n",
            "    Found existing installation: Flask 1.1.4\n",
            "    Uninstalling Flask-1.1.4:\n",
            "      Successfully uninstalled Flask-1.1.4\n",
            "Successfully installed flask-0.12.2\n"
          ]
        }
      ],
      "source": [
        "!pip install aitextgen\n",
        "!pip install flask-ngrok\n",
        "!pip install flask==0.12.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#Visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "sns.set(rc = {'figure.figsize':(35,10)})\n",
        "sns.set_palette(\"Paired\")\n",
        "sns.set_style(\"white\")\n",
        "from flask import Flask, request, render_template\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "##########Models\n",
        "from aitextgen import aitextgen\n",
        "import torch\n",
        "import sys\n",
        "import os\n",
        "import random\n",
        "import time \n",
        "import datetime\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    current_device = torch.cuda.current_device()\n",
        "    print(\"Available GPUs: \", torch.cuda.get_device_name(current_device))\n",
        "    print()\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config, AdamW, get_linear_schedule_with_warmup, GPT2TokenizerFast\n",
        "from torch.utils.data import Dataset, random_split, DataLoader, RandomSampler, SequentialSampler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAInlBf8TN3_",
        "outputId": "1384b9e3-cafb-4a00-a1d7-f11af23489de"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available GPUs:  Tesla T4\n",
            "\n",
            "Sun Jul  3 15:18:02 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   66C    P8    11W /  70W |      3MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Data"
      ],
      "metadata": {
        "id": "UM66iavGf3aQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col_poem = \"stanza_text\"\n",
        "\n",
        "#------------------------------------------------\n",
        "\n",
        "df_poems = pd.read_csv(\"/content/drive/MyDrive/proyecto_NLP/data/poe_poems_stanzas.csv\")\n",
        "df_poems = df_poems[(df_poems[col_poem].notna()) & (df_poems[col_poem]!=\" \")]\n",
        "\n",
        "df_poems = df_poems.drop(137, axis=0)\n",
        "print(\"Dimension of datase:\", df_poems.shape)"
      ],
      "metadata": {
        "id": "Cj-VEZsSTvgj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fd277a3-f262-4451-a68b-8fd5a1e48854"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimension of datase: (214, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##📜 Global Functions"
      ],
      "metadata": {
        "id": "vbAy8fYFjuTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PoemDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, data, tokenizer, max_length, gpt2_type='gpt2'):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "        \n",
        "        for i in data:\n",
        "          \n",
        "            encodings_dict = tokenizer('<BOS>' + i + '<EOS>',\n",
        "                                     truncation=True,\n",
        "                                     max_length=max_length,\n",
        "                                     padding='max_length'\n",
        "                                    )\n",
        "\n",
        "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "\n",
        "    def __len__(self):\n",
        "        \n",
        "        return len(self.input_ids)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        return self.input_ids[idx], self.attn_masks[idx]\n",
        "\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def format_time(elapsed):\n",
        "    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n",
        "    \n",
        "\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def train_val_split(split, dataset):\n",
        "\n",
        "    train_size = int(split * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    return train_size, val_size\n",
        "\n"
      ],
      "metadata": {
        "id": "a0q2WQhsjwuX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "RANDOM_SEED = 73\n",
        "BATCH_SIZE = 2\n",
        "epochs = 20\n",
        "MAX_LEN = 1024\n",
        "home_directory = \"/content/drive/MyDrive/proyecto_NLP/models\"\n",
        "###################################################################################################################################################################################################\n",
        "\n",
        "pretrained_weights = 'gpt2' ## as over 1.5 billion parameters\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_weights)\n",
        "\n",
        "special_tokens_dict = {'bos_token': '<BOS>', 'eos_token': '<EOS>', 'pad_token': '<PAD>'}\n",
        "num_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6y4DMPmrj82",
        "outputId": "43eecfe6-2511-4d53-a683-5e1b57fff511"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "Assigning <BOS> to the bos_token key of the tokenizer\n",
            "Adding <BOS> to the vocabulary\n",
            "Assigning <EOS> to the eos_token key of the tokenizer\n",
            "Adding <EOS> to the vocabulary\n",
            "Assigning <PAD> to the pad_token key of the tokenizer\n",
            "Adding <PAD> to the vocabulary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🕸 Text Generation - GPT-2"
      ],
      "metadata": {
        "id": "s99fEDcBgTk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_poems = df_poems.groupby(['title'])[col_poem].transform(lambda x: ''.join(x)).drop_duplicates().reset_index(drop=True)"
      ],
      "metadata": {
        "id": "gV0QRdMehDro"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_poem_length = max([len(tokenizer.encode(poem)) for poem in combined_poems])\n",
        "min_poem_length = min([len(tokenizer.encode(poem)) for poem in combined_poems])\n",
        "print('Longest Poem:', max_poem_length, 'tokens long.')\n",
        "print('Shortest Poem:', min_poem_length, 'tokens long.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk_vZr0jhXSi",
        "outputId": "27fd86ec-4f34-48fc-9dd0-5e0bad95252c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1668 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longest Poem: 4427 tokens long.\n",
            "Shortest Poem: 55 tokens long.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stanza_length = [len(tokenizer.encode(stanza)) for stanza in df_poems[col_poem].values]\n",
        "max_stanza_length = max(stanza_length)\n",
        "min_stanza_length = min(stanza_length)\n",
        "print('Number of stanzas longer than max length: ', sum([st_len > MAX_LEN for st_len in stanza_length])) \n",
        "print('Longest Stanza:', max_stanza_length, 'tokens long.')\n",
        "print('Shortest Stanza:', min_stanza_length, 'tokens long.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apZtigEeiW2v",
        "outputId": "2fa3c562-9c19-4886-d94c-64958bd7398b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of stanzas longer than max length:  0\n",
            "Longest Stanza: 875 tokens long.\n",
            "Shortest Stanza: 15 tokens long.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "poem_dataset = PoemDataset(df_poems[col_poem].values, tokenizer, max_length=MAX_LEN)"
      ],
      "metadata": {
        "id": "tKEq4EmgjVkH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##🐝 Train-Test Split"
      ],
      "metadata": {
        "id": "O5sEf3nPoTq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "poem_train_size, poem_val_size = train_val_split(0.8, poem_dataset)\n",
        "\n",
        "# random split imported from troch.utils\n",
        "poem_train_dataset, poem_val_dataset = random_split(poem_dataset, [poem_train_size, poem_val_size])\n",
        "\n",
        "\n",
        "#-------------------------------------------------------Random Seeds\n",
        "\n",
        "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lHya30gkKz6",
        "outputId": "fdd3e71a-cb25-4a6d-c4fc-2ae4c1b5de72"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f3713c25570>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##🐝 Data Loaders"
      ],
      "metadata": {
        "id": "HAPt-zIgp1CT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poem_train_dataloader = DataLoader(poem_train_dataset,\n",
        "                              sampler=RandomSampler(poem_train_dataset),\n",
        "                              batch_size=BATCH_SIZE)\n",
        "\n",
        "poem_val_dataloader = DataLoader(poem_val_dataset,\n",
        "                            sampler=SequentialSampler(poem_val_dataset),\n",
        "                            batch_size=BATCH_SIZE)\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# helper function for logging time\n",
        "def format_time(elapsed):\n",
        "    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n",
        "\n",
        "# hyperparameters\n",
        "learning_rate = 1e-4\n",
        "eps = 1e-8\n",
        "warmup_steps = 35\n",
        "# this produces sample output every 100 steps\n",
        "sample_every = 50\n",
        "\n",
        "# create text generation seed prompt\n",
        "device = torch.device('cuda')\n",
        "\n",
        "prompt = \"<BOS>\"\n",
        "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "generated = generated.to(device)"
      ],
      "metadata": {
        "id": "YyLRR5WWkhve"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##🚀 FineTunning: Training"
      ],
      "metadata": {
        "id": "w5pNjrn1rLwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configuration = GPT2Config(vocab_size=len(tokenizer), n_positions=MAX_LEN).from_pretrained('gpt2', output_hidden_states=True)\n",
        "\n",
        "poem_model = GPT2LMHeadModel.from_pretrained('gpt2', config=configuration)\n",
        "\n",
        "poem_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "poem_model.cuda()\n",
        "\n",
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "optimizer = AdamW(poem_model.parameters(), lr=learning_rate, eps=eps)\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "total_steps = len(poem_train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "# This changes the learning rate as the training loop progresses\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=warmup_steps,\n",
        "                                            num_training_steps=total_steps)\n",
        "\n",
        "#------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7p0KsD0qDS-",
        "outputId": "1d3d66d2-9bae-4b97-a422-c0c79a4a646e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_t0 = time.time()\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "\n",
        "training_stats = []\n",
        "\n",
        "\n",
        "poem_model = poem_model.to(device)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "\n",
        "poem_model = poem_model.to(device)\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    total_train_loss = 0\n",
        "\n",
        "    poem_model.train()\n",
        "\n",
        "    for step, batch in enumerate(poem_train_dataloader):\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[0].to(device)\n",
        "        b_masks = batch[1].to(device)\n",
        "\n",
        "        poem_model.zero_grad()        \n",
        "\n",
        "        outputs = poem_model(b_input_ids,\n",
        "                          labels=b_labels, \n",
        "                          attention_mask = b_masks,\n",
        "                          token_type_ids=None\n",
        "                        )\n",
        "\n",
        "        loss = outputs[0]  \n",
        "\n",
        "        batch_loss = loss.item()\n",
        "        total_train_loss += batch_loss\n",
        "\n",
        "        # Get sample every x batches.\n",
        "        if step % sample_every == 0 and not step == 0:\n",
        "\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(poem_train_dataloader), batch_loss, elapsed))\n",
        "\n",
        "            poem_model.eval()\n",
        "\n",
        "            sample_outputs = poem_model.generate(\n",
        "                                    bos_token_id= random.randint(1,30000),\n",
        "                                    do_sample=True,   \n",
        "                                    top_k=50, \n",
        "                                    max_length = MAX_LEN,\n",
        "                                    top_p=0.95, \n",
        "                                    num_return_sequences=1\n",
        "                                )\n",
        "            for i, sample_output in enumerate(sample_outputs):\n",
        "                  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
        "            poem_model.train()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(poem_train_dataloader)       \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================entario\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    poem_model.eval()\n",
        "\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in poem_val_dataloader:\n",
        "        \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[0].to(device)\n",
        "        b_masks = batch[1].to(device)\n",
        "        \n",
        "        with torch.no_grad():        \n",
        "\n",
        "            outputs  = poem_model(b_input_ids, \n",
        "#                            token_type_ids=None, \n",
        "                             attention_mask = b_masks,\n",
        "                            labels=b_labels)\n",
        "          \n",
        "            loss = outputs[0]  \n",
        "            \n",
        "        batch_loss = loss.item()\n",
        "        total_eval_loss += batch_loss        \n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(poem_val_dataloader)\n",
        "    \n",
        "    validation_time = format_time(time.time() - t0)    \n",
        "\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "\n",
        "torch.save(poem_model.state_dict(), home_directory + 'poem_stanza_model.pth')"
      ],
      "metadata": {
        "id": "o1a8iqGPrwOt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1a5dfaf-f7ed-4c4e-ac6e-dc2a79631b98"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training complete!\n",
            "Total training took 0:00:00 (h:mm:ss)\n",
            "\n",
            "======== Epoch 1 / 20 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    50  of     86. Loss: 1.3012336492538452.   Elapsed: 0:00:36.\n",
            "0:  Poké2\n",
            "\n",
            "  Average training loss: 2.07\n",
            "  Training epoch took: 0:01:13\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.68\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 2 / 20 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    50  of     86. Loss: 0.5000213980674744.   Elapsed: 0:00:36.\n",
            "0: avens\n",
            "\n",
            "  Average training loss: 0.55\n",
            "  Training epoch took: 0:01:13\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.65\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 3 / 20 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    50  of     86. Loss: 0.9047629237174988.   Elapsed: 0:00:37.\n",
            "0:  Erd, and her crown.\n",
            "\n",
            "  Average training loss: 0.51\n",
            "  Training epoch took: 0:01:13\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.64\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 4 / 20 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    50  of     86. Loss: 0.37863612174987793.   Elapsed: 0:00:37.\n",
            "0:  sensors\n",
            "\n",
            "Familiar bells\n",
            "\n",
            "Are drawn by the air\n",
            "\n",
            "Familiar bells\n",
            "\n",
            "Are drawn by the air\n",
            "\n",
            "\n",
            "This, this, this, this\n",
            "\n",
            " this, this\n",
            "\n",
            " this, this\n",
            "\n",
            " this, this, this, this\n",
            "\n",
            "\n",
            "this, this, this\n",
            " this, this, this, this\n",
            "\n",
            "\n",
            "this, this, this, this\n",
            "\n",
            "this, this\n",
            "\n",
            "this, this\n",
            "\n",
            "this, this\n",
            "\n",
            "this, this, this\n",
            "\n",
            "this, this\n",
            "\n",
            "—\n",
            "—\n",
            " \n",
            "The first, the second—\n",
            " \n",
            "\n",
            "the first—\n",
            "\n",
            " the second—\n",
            " \n",
            "the second—\n",
            " \n",
            "the second—\n",
            "\n",
            "\n",
            " \n",
            "\n",
            " The first, the second—\n",
            " \n",
            "the second—\n",
            " \n",
            "\n",
            "the second—\n",
            " \n",
            "The first—\n",
            " \n",
            " \n",
            "— \n",
            " \n",
            " \n",
            " \n",
            "\n",
            " \n",
            " \n",
            " \n",
            "\n",
            " \n",
            " \n",
            " \n",
            "— \n",
            "   \n",
            "— \n",
            "   \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "— \n",
            "   \n",
            "  \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "  \n",
            "   \n",
            " \n",
            "  \n",
            " \n",
            " \n",
            "  \n",
            "    \n",
            "  \n",
            " \n",
            "  \n",
            " \n",
            "    \n",
            "  \n",
            " \n",
            "  \n",
            " \n",
            " \n",
            "    \n",
            " \n",
            "  \n",
            "  \n",
            "  \n",
            " \n",
            " \n",
            "    \n",
            " \n",
            "  \n",
            "   \n",
            "  \n",
            " \n",
            "    \n",
            "  \n",
            " \n",
            " \n",
            "     \n",
            "   \n",
            "        \n",
            "  \n",
            "   \n",
            "   \n",
            "   \n",
            " \n",
            "   \n",
            "   \n",
            "   \n",
            "    \n",
            "     \n",
            "     \n",
            "      \n",
            "    \n",
            "   \n",
            "      \n",
            "  \n",
            "       \n",
            "   \n",
            "     \n",
            "     \n",
            "    \n",
            "      \n",
            "      \n",
            "       \n",
            "      \n",
            "    \n",
            "         \n",
            "     \n",
            "    \n",
            "   \n",
            "    \n",
            "      \n",
            "      \n",
            "       \n",
            "       \n",
            "      \n",
            "     \n",
            "       \n",
            "      \n",
            "        \n",
            "      \n",
            "      \n",
            "    \n",
            "     \n",
            "         \n",
            "    \n",
            "       \n",
            "      \n",
            "      \n",
            "     \n",
            "       \n",
            "    \n",
            "        \n",
            "       \n",
            "      \n",
            "      \n",
            "        \n",
            "        \n",
            "        \n",
            "         \n",
            "        \n",
            "        \n",
            "         \n",
            "        \n",
            "        \n",
            "        \n",
            "       \n",
            "         \n",
            "        \n",
            "            \n",
            "        \n",
            "           \n",
            "           \n",
            "          \n",
            "           \n",
            "          \n",
            "          \n",
            "           \n",
            "            \n",
            "           \n",
            "        \n",
            "          \n",
            "             \n",
            "           \n",
            " \n",
            "\n",
            "  Average training loss: 0.49\n",
            "  Training epoch took: 0:01:13\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.63\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 5 / 20 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    50  of     86. Loss: 0.6465129256248474.   Elapsed: 0:00:36.\n",
            "0: atilityThe winds are ringing, that seem to be ringing—\n",
            "And you are struggling—but not yet,\n",
            "And you feel the danger of waking\n",
            "And dreaming. Note Note Note Note\n",
            " Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epoch took: 0:01:13\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.64\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 6 / 20 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    50  of     86. Loss: 0.3684578835964203.   Elapsed: 0:00:36.\n",
            "0:  heav\n",
            "Into the air is the voice of all living things\n",
            "Of a strange world in dreams\n",
            "Of a dream to be \n",
            "With the same passion I heard, \n",
            "A lone eagle, whose wings were golden \n",
            "Whose flight was all-round \n",
            "And whose flight was all-round \n",
            "Into a strange star\n",
            "And its winds are the terror of the seas—\n",
            "And its stars are the wildest \n",
            "Their fears of the sea \n",
            "And those who dwell in the woods\n",
            "Are the harlots of the mountain \n",
            "And the wild-clowns of the moose \n",
            "Who keep my hair—\n",
            "And the misty night in my room \n",
            "And the gray mist in my bed \n",
            "Are the fears of the sea \n",
            "My fears of the night \n",
            "Have passed and gone out \n",
            "And the dread that fell upon my bed \n",
            "My fears that were still \n",
            "Of waking, and my fears that were still \n",
            "Whereby I may well cherish \n",
            "The memories of life—which I cherish to this day—\n",
            "For I cherish this life no less—I cherish not only the things that I have \n",
            "While it is written, the things that I have \n",
            "That I have read in my childhood and \n",
            "And have felt deeply\n",
            "Thou hast loved—as well as the things that you know \n",
            "Of my childhood and the things that you love,\n",
            "In what you are—that, while you are here \n",
            "And while you think of me, you are dreaming.\n",
            "The fears of the night \n",
            "Are mine—these fears I feel—the fears I feel \n",
            "In the hours of night \n",
            "Where I am feeling the winds on my bed \n",
            "At the sounds, that I may well tremble—\n",
            "\n",
            "A theme that I have sung—and no syllable syllable \n",
            "Is uttered to this world—is mine. Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note Note\n",
            "\n",
            "  Average training loss: 0.45\n",
            "  Training epoch took: 0:01:13\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.63\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 7 / 20 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    50  of     86. Loss: 0.4025140702724457.   Elapsed: 0:00:36.\n",
            "0:  DannyIn thine own voice\n",
            " But in her own song,\n",
            " On its very wings she trembled \n",
            " And whispered her love—the theme of her beauty—\n",
            " By her melodies, I may safely\n",
            "\n",
            "  Average training loss: 0.43\n",
            "  Training epoch took: 0:01:12\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.63\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 8 / 20 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    50  of     86. Loss: 0.33963412046432495.   Elapsed: 0:00:36.\n",
            "0:  AmongSome who are dead and buried\n",
            " Of my spirit-filled waters, \n",
            "\n",
            "Whose waters have not thine eyes,\n",
            "Their shore-swept skies, \n",
            "And whose flowers have not died!\n",
            "They were not the light of life \n",
            "Though they stood upright in the garden\n",
            "And wore the stars:\n",
            "Their fountains were the light of glory. \n",
            "For all these days I weep \n",
            "With tears that will not go \n",
            "Unwritten\n",
            "That my heart—and that my spirit—\n",
            "Is not driven \n",
            "To weep—\n",
            "By a mad passion of the heart! \n",
            "For it will lie, \n",
            "By a mad desire of the head—\n",
            "It will be a dream.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0:01:12\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.63\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 9 / 20 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    50  of     86. Loss: 0.41341373324394226.   Elapsed: 0:00:36.\n",
            "0: avorite-at-midnight,\n",
            " Ah, though, my friends: my sorrow in her,\n",
            " Unadulterated,\n",
            " She dined, with those she loved,\n",
            " Like the last year and a half \n",
            " In this world of mine.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0:01:12\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.63\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 10 / 20 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    50  of     86. Loss: 0.32568642497062683.   Elapsed: 0:00:36.\n",
            "0: cialAnd then said, \"Ah! Why didst Thou, O God, teach these things? Thou whoest a lord?\" Ah! let us not see that in these things, that thy angels, whom thou shouldst envy, didst not descend to the light of the Sun, though in Thy sight were the Earth illumined, and then went about dreaming on a throne of light and beauty—with a star in his right hand \n",
            " The Goddess was born to them, and bore them to their eternal life!\n",
            "\n",
            "  Average training loss: 0.40\n",
            "  Training epoch took: 0:01:12\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.63\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 11 / 20 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    50  of     86. Loss: 0.4681466221809387.   Elapsed: 0:00:36.\n",
            "0: anyahu:—\n",
            " \"There is no time!\n",
            "The Future Is Past!\"\n",
            "How can you feel it?\n",
            "There are no hopes,\n",
            "Too feeble to hope,\n",
            "Too feeble to aspire—\n",
            "Too feeble—not only to seek\n",
            "To some remote future,\n",
            "But to see, in dreams,\n",
            "Into dreams of deep thought.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0:01:12\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.64\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 12 / 20 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    50  of     86. Loss: 0.5876380801200867.   Elapsed: 0:00:36.\n",
            "0: Group. name. ( \"Mystique de l\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epoch took: 0:01:12\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.64\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 13 / 20 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    50  of     86. Loss: 0.42285630106925964.   Elapsed: 0:00:36.\n",
            "0:  BeerThy eyes \n",
            " A ghastly blush \n",
            " Is down on my face, like a rosemary plant \n",
            " In flowers that were never before \n",
            " And thus my spirit died—\n",
            " And my heart never breathed\n",
            " My spirit never passed,\n",
            " Never—never—never—\n",
            " As a feeling of joy on a burning July eve \n",
            " (Though my heart—\n",
            " So let it be) \n",
            " My spirit is dead—\n",
            " A melancholy dream, a sinking feeling \n",
            " Of my melancholyness as if that spirit \n",
            " Had fallen within the breast of an eagle \n",
            " So that its heart was upon a thousand lilies, \n",
            " And a ghastly blush \n",
            " Is down on my face, like a plant that was never before—\n",
            " A ghoul's heart is on my heart as a burning flower \n",
            " In flowers that were never before \n",
            " And thus my spirit died— \n",
            " A dream in hopes that my spirit may never leave my presence—\n",
            " A melancholy dream in hopes that my spirit may never return \n",
            " Note Contents\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0:01:12\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.64\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 14 / 20 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    50  of     86. Loss: 1.110690951347351.   Elapsed: 0:00:36.\n",
            "0: iasmAt length, the moon turned, And from its upturn'd rim, sprang a bird That thronged, till it soared above\n",
            " At the tops of the gaudy walls of Raven's Cove. 1827 Contents p. 2\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0:01:13\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.65\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 15 / 20 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    50  of     86. Loss: 0.3335363566875458.   Elapsed: 0:00:36.\n",
            "0:  prosecutThen I saw the angels, sitting so high above,\n",
            "And they would not let me go, \n",
            "When the angelials trailed behind me, \n",
            "From their way of flying,\n",
            "From their way of repose\n",
            "By the swiftest of those naphthalines.\n",
            "For at my hand I held\n",
            "A wand of Fate, a gift, \n",
            "And to me a token of my gratitude. 1849 Contents p. 3\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0:01:12\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.65\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 16 / 20 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    50  of     86. Loss: 0.5708314776420593.   Elapsed: 0:00:36.\n",
            "0: mouthAt thine isle, \n",
            " In thy greenest name \n",
            " And the solemn wreath \n",
            " Which floats above thy holier dell\n",
            " Are the roses, my dear Love! O, her Love! \n",
            " Thy perfume is a symbol \n",
            " Of love that binds \n",
            " Its prey, even if that prey \n",
            " Is itself the idol!\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0:01:12\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.66\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 17 / 20 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    50  of     86. Loss: 0.2291187345981598.   Elapsed: 0:00:36.\n",
            "0: owersThe bells\n",
            "That the choir bells, by their own gurgles,\n",
            "Gurgled wildly about their high-mouthed,\n",
            "Rendered from the distant hills, \n",
            "Rendered from the same source that thine\n",
            "Bastards from the same town had ridden\n",
            "In the same troop,\n",
            "In the same troop, and in the same troop,\n",
            "Rendered from the same source that thine own\n",
            "Beloved, the good Lenore of Rome, \n",
            "Rendered from the same source that thine own home had been \n",
            "In the same tomb of Ulal III. \n",
            "Rendered from the same source that Thante himself had given\n",
            "To his mother, whose dews were made\n",
            "On a velvet couch of velvet—\n",
            "And whose chains he bound,\n",
            "On a velvet velvet bed of velvet—\n",
            "And whose girdles he bound, \n",
            "On a velvet curtain that bound all that slept \n",
            "On a velvet bed of velvet.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0:01:13\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.66\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 18 / 20 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    50  of     86. Loss: 0.5555798411369324.   Elapsed: 0:00:36.\n",
            "0:  commalmed\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0:01:12\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.66\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 19 / 20 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    50  of     86. Loss: 0.18550468981266022.   Elapsed: 0:00:36.\n",
            "0: nesota\n",
            "\n",
            "When that day came (\n",
            " Where the night shall lie)\n",
            " And the angels cheered on,\n",
            " That the pallid air\n",
            " And the angels cheered on, \n",
            " And the birds cheered on,\n",
            " That the gray-haired bees cheered \n",
            " On the reared pheasants \n",
            " And the lilies cheered, and the bees cheered,\n",
            " But all that night—\n",
            " All that night—\n",
            " In all that night \n",
            " I feel as if the sky \n",
            " Is a pallid night, and the stars \n",
            " Chime in the woofing sound,\n",
            " And the murmured moaning of the bees \n",
            " Are the accompaniment to the moaning and the wailing.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0:01:12\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.66\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 20 / 20 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    50  of     86. Loss: 0.18914002180099487.   Elapsed: 0:00:36.\n",
            "0:  waitedThere were murmuring beside me as if dreaming; \n",
            "And when, then, suddenly, \n",
            "(like things in dreams, \n",
            "Dim in tone)\n",
            "The moon rose and died—\n",
            "And from a deep pride\n",
            "A maiden of Earth—\n",
            "Fell in beauty, and from a chilly pride\n",
            "Fell within me like a cloud.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0:01:12\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.66\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:25:53 (h:mm:ss)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 👨‍🚀 Summary of the training process"
      ],
      "metadata": {
        "id": "EKkI6RKJyWnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "metadata": {
        "id": "KRupsj2jyWeQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "outputId": "16126033-35db-466b-c894-65bfd746fd77"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Training Loss  Valid. Loss Training Time Validation Time\n",
              "epoch                                                          \n",
              "1               2.07         0.68       0:01:13         0:00:05\n",
              "2               0.55         0.65       0:01:13         0:00:05\n",
              "3               0.51         0.64       0:01:13         0:00:05\n",
              "4               0.49         0.63       0:01:13         0:00:05\n",
              "5               0.47         0.64       0:01:13         0:00:05\n",
              "6               0.45         0.63       0:01:13         0:00:05\n",
              "7               0.43         0.63       0:01:12         0:00:05\n",
              "8               0.42         0.63       0:01:12         0:00:05\n",
              "9               0.42         0.63       0:01:12         0:00:05\n",
              "10              0.40         0.63       0:01:12         0:00:05\n",
              "11              0.39         0.64       0:01:12         0:00:05\n",
              "12              0.38         0.64       0:01:12         0:00:05\n",
              "13              0.37         0.64       0:01:12         0:00:05\n",
              "14              0.36         0.65       0:01:13         0:00:05\n",
              "15              0.35         0.65       0:01:12         0:00:05\n",
              "16              0.34         0.66       0:01:12         0:00:05\n",
              "17              0.34         0.66       0:01:13         0:00:05\n",
              "18              0.34         0.66       0:01:12         0:00:05\n",
              "19              0.33         0.66       0:01:12         0:00:05\n",
              "20              0.33         0.66       0:01:12         0:00:05"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-40cc9b78-fd2f-4e79-ae24-37b8748fe7f4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.07</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0:01:13</td>\n",
              "      <td>0:00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.55</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0:01:13</td>\n",
              "      <td>0:00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.51</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0:01:13</td>\n",
              "      <td>0:00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.49</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0:01:13</td>\n",
              "      <td>0:00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.47</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0:01:13</td>\n",
              "      <td>0:00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.45</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0:01:13</td>\n",
              "      <td>0:00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.43</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0:01:12</td>\n",
              "      <td>0:00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.42</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0:01:12</td>\n",
              "      <td>0:00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.42</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0:01:12</td>\n",
              "      <td>0:00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.40</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0:01:12</td>\n",
              "      <td>0:00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.39</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0:01:12</td>\n",
              "      <td>0:00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.38</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0:01:12</td>\n",
              "      <td>0:00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.37</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0:01:12</td>\n",
              "      <td>0:00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.36</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0:01:13</td>\n",
              "      <td>0:00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.35</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0:01:12</td>\n",
              "      <td>0:00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.34</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0:01:12</td>\n",
              "      <td>0:00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.34</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0:01:13</td>\n",
              "      <td>0:00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.34</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0:01:12</td>\n",
              "      <td>0:00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.33</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0:01:12</td>\n",
              "      <td>0:00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.33</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0:01:12</td>\n",
              "      <td>0:00:05</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-40cc9b78-fd2f-4e79-ae24-37b8748fe7f4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-40cc9b78-fd2f-4e79-ae24-37b8748fe7f4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-40cc9b78-fd2f-4e79-ae24-37b8748fe7f4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 👨‍🚀 Performance Summary"
      ],
      "metadata": {
        "id": "VGqxIrB50D2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4, 5, 6, 7,8,9,10,11,12,13,14,15,16,17,19,20])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "bogzbMxlvHZM",
        "outputId": "29ac21ed-449c-4ab2-e865-6ebbcf763f6c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##🌌 Model Info"
      ],
      "metadata": {
        "id": "bMzwiVH5yl4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(poem_model.named_parameters())\n",
        "\n",
        "print('The GPT-2 model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:2]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[2:14]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-2:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "metadata": {
        "id": "WDpr7xdD0dpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81fb1bc5-e45e-429a-f7c5-cfdbe65e2cdd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The GPT-2 model has 148 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "transformer.wte.weight                                  (50260, 768)\n",
            "transformer.wpe.weight                                   (1024, 768)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "transformer.h.0.ln_1.weight                                   (768,)\n",
            "transformer.h.0.ln_1.bias                                     (768,)\n",
            "transformer.h.0.attn.c_attn.weight                       (768, 2304)\n",
            "transformer.h.0.attn.c_attn.bias                             (2304,)\n",
            "transformer.h.0.attn.c_proj.weight                        (768, 768)\n",
            "transformer.h.0.attn.c_proj.bias                              (768,)\n",
            "transformer.h.0.ln_2.weight                                   (768,)\n",
            "transformer.h.0.ln_2.bias                                     (768,)\n",
            "transformer.h.0.mlp.c_fc.weight                          (768, 3072)\n",
            "transformer.h.0.mlp.c_fc.bias                                (3072,)\n",
            "transformer.h.0.mlp.c_proj.weight                        (3072, 768)\n",
            "transformer.h.0.mlp.c_proj.bias                               (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "transformer.ln_f.weight                                       (768,)\n",
            "transformer.ln_f.bias                                         (768,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Saving & Loading Fine-Tuned Model\n"
      ],
      "metadata": {
        "id": "KMaFNHYw0Jkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = home_directory +'/model_save/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = poem_model.module if hasattr(poem_model, 'module') else poem_model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "metadata": {
        "id": "2qxncMRnyvae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36a91ac3-fce0-4ffe-fe0a-af78dcc2b38d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model to /content/drive/MyDrive/proyecto_NLP/models/model_save/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/drive/MyDrive/proyecto_NLP/models/model_save/config.json\n",
            "Model weights saved in /content/drive/MyDrive/proyecto_NLP/models/model_save/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/proyecto_NLP/models/model_save/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/proyecto_NLP/models/model_save/special_tokens_map.json\n",
            "added tokens file saved in /content/drive/MyDrive/proyecto_NLP/models/model_save/added_tokens.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/proyecto_NLP/models/model_save/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/proyecto_NLP/models/model_save/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/proyecto_NLP/models/model_save/vocab.json',\n",
              " '/content/drive/MyDrive/proyecto_NLP/models/model_save/merges.txt',\n",
              " '/content/drive/MyDrive/proyecto_NLP/models/model_save/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "home_directory+'/model_save/'"
      ],
      "metadata": {
        "id": "OYg_S0691HjR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "b6b95547-b615-4120-db82-ee7f381786e4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/proyecto_NLP/models/model_save/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l --block-size=K /content/drive/MyDrive/proyecto_NLP/models/model_save/"
      ],
      "metadata": {
        "id": "EsObwP6r07Ob",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85c3bb6f-331b-4b42-b2ba-0184d84de165"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 499869K\n",
            "-rw------- 1 root root      1K Jul  3 15:45 added_tokens.json\n",
            "-rw------- 1 root root      1K Jul  3 15:45 config.json\n",
            "-rw------- 1 root root    446K Jul  3 15:45 merges.txt\n",
            "-rw------- 1 root root 498444K Jul  3 15:45 pytorch_model.bin\n",
            "-rw------- 1 root root      1K Jul  3 15:45 special_tokens_map.json\n",
            "-rw------- 1 root root      1K Jul  3 15:45 tokenizer_config.json\n",
            "-rw------- 1 root root    976K Jul  3 15:45 vocab.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l --block-size=M /content/drive/MyDrive/proyecto_NLP/models/model_save/pytorch_model.bin"
      ],
      "metadata": {
        "id": "OFtW-6ka1BUK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "938d63aa-645a-426b-d8d1-92c719dcca75"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw------- 1 root root 487M Jul  3 15:45 /content/drive/MyDrive/proyecto_NLP/models/model_save/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy the model files to a directory in your Google Drive.\n",
        "#!cp -r /content/drive/MyDrive/proyecto_NLP/models/model_save/  $data_dir\n",
        "\n",
        "# # Load a trained model and vocabulary that you have fine-tuned\n",
        "#model = GPT2LMHeadModel.from_pretrained(output_dir)\n",
        "#tokenizer = GPT2Tokenizer.from_pretrained(output_dir)\n",
        "#model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAb9n45p1Wk0",
        "outputId": "9e10e8ed-b917-40c8-9764-b15a343f0fec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: missing destination file operand after '/content/drive/MyDrive/proyecto_NLP/models/model_save/'\n",
            "Try 'cp --help' for more information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Poems Generation"
      ],
      "metadata": {
        "id": "NoydP2C61uXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poem_model.eval()\n",
        "\n",
        "prompt = 'love is'\n",
        "\n",
        "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "generated = generated.to(device)\n",
        "\n",
        "sample_outputs = poem_model.generate(\n",
        "                                generated, \n",
        "                                do_sample=True,   \n",
        "                                top_k=40, \n",
        "                                max_length = 100,\n",
        "                                top_p=0.95, \n",
        "                                num_return_sequences=3,\n",
        "                                num_beams=5, no_repeat_ngram_size=4, early_stopping=True\n",
        "                                )\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAKb4iCOxMxf",
        "outputId": "5dcfa4a8-86de-47a1-c011-89a5664e290f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: love is the light of the night—\n",
            " And the shadow of the mountain \n",
            " And the mist of the sea \n",
            " Is a barrier—\n",
            " And a barrier that will not be broken \n",
            " By the battle-cry \n",
            " Of the trumpet-thunder-thunder,\n",
            " But by the roar of the battle \n",
            " Shall be the battle cry of the dead \n",
            " And shall be the cry of the living—\n",
            " And by the roar shall be the roar\n",
            " Of the battle-\n",
            "\n",
            "\n",
            "1: love is the light of life;\n",
            "It glows in the dim of night;\n",
            "It flickers with the light of day,\n",
            "And in the mist of day, \n",
            "It trembles with the chilly eye \n",
            "Of the lonesome and the dead.\n",
            "\n",
            "\n",
            "2: love is a synonym for love,\n",
            " For love is a double-edged sword—\n",
            " And love is a gift from God—\n",
            " For the love of God is a gift of life—\n",
            " And the love of man is a gift given by God—\n",
            " And we are saved only by the grace of God.\"\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "poem_model.eval()\n",
        "\n",
        "prompt = 'heaven is good'\n",
        "\n",
        "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "generated = generated.to(device)\n",
        "\n",
        "sample_outputs = poem_model.generate(\n",
        "                                generated, \n",
        "                                do_sample=True,   \n",
        "                                top_k=40, \n",
        "                                max_length = 150,\n",
        "                                top_p=0.95, \n",
        "                                num_return_sequences=3,\n",
        "                                num_beams=5, no_repeat_ngram_size=4, early_stopping=True\n",
        "                                )\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "id": "d62W2SzcSpza",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f16f2d7-9a6a-4536-cd58-b2b76c135240"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: heaven is good, and light is light;\n",
            "And all things which are holy, \n",
            "Are light, and all things which seem \n",
            "To be a gift from God to us, \n",
            "And to us a token of our gratitude. 18\n",
            "¹ ¿ 18\n",
            "\n",
            "\n",
            "1: heaven is good,\n",
            "And the winds are gentle, \n",
            "And the skies are clear—\n",
            "And the stars seem to be smiling—\n",
            "And there is no need \n",
            "To sing the hymns of the hymn-winds, \n",
            "Unless, perhaps, you are a poet or a hymn-stringer—\n",
            "And you will sing the hymn of the sun-ray, \n",
            "Of the moon-ray,\n",
            "Of the stars,\n",
            "And there will be no need\n",
            "For you to sing the song of the moon, \n",
            " Of the stars, or of the angels, \n",
            "With the accompaniment of the bells and the bells of the bells, \n",
            "So that you may feel no need\n",
            "\n",
            "\n",
            "2: heaven is good, \n",
            " And all things are light, \n",
            " That is, I believe, the reason \n",
            " That all things are bright—\n",
            " And all that is gray—\n",
            " That all that is sad, \n",
            " Is the reason that all things seem \n",
            " To be of one essence, \n",
            " But that all things are of one essence—\n",
            " And that all things—all things—have one essence— \n",
            " The reason why all things are \n",
            " One and the same—\n",
            " Why all things are one and the same \n",
            " And why all things—and all things—have a single essence \n",
            " That alone can be comprehended \n",
            " By reason alone—\n",
            " And yet all things have a single essence—\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dgP6WqTk0sbg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}